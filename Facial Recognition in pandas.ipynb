{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-27T12:13:56.729786Z","iopub.execute_input":"2022-05-27T12:13:56.730248Z","iopub.status.idle":"2022-05-27T12:13:56.735078Z","shell.execute_reply.started":"2022-05-27T12:13:56.730210Z","shell.execute_reply":"2022-05-27T12:13:56.734241Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Convolution2D,MaxPooling2D,Flatten,Conv2D,Dropout\nfrom keras.preprocessing.image import ImageDataGenerator\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tensorflow as tf\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:14:00.713233Z","iopub.execute_input":"2022-05-27T12:14:00.713603Z","iopub.status.idle":"2022-05-27T12:14:04.865377Z","shell.execute_reply.started":"2022-05-27T12:14:00.713564Z","shell.execute_reply":"2022-05-27T12:14:04.864645Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Using mtcnn to detect faces - then facenet-keras to get a face embedding - finally using decision tree classifier on the produced dataset","metadata":{}},{"cell_type":"code","source":"# view the data\n\nim=Image.open('../input/yale-face-database/subject09.leftlight')\n\nim","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:14:08.654380Z","iopub.execute_input":"2022-05-27T12:14:08.654661Z","iopub.status.idle":"2022-05-27T12:14:08.706487Z","shell.execute_reply.started":"2022-05-27T12:14:08.654629Z","shell.execute_reply":"2022-05-27T12:14:08.705795Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# # loading kaeras facenet model\n# from keras.models import load_model\n# # load the model\n# model = load_model('../input/facenet-keras/facenet_keras.h5')\n# # summarize input and output shape\n# print(model.inputs)\n# print(model.outputs)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T19:23:05.797007Z","iopub.execute_input":"2022-05-26T19:23:05.797346Z","iopub.status.idle":"2022-05-26T19:23:11.071591Z","shell.execute_reply.started":"2022-05-26T19:23:05.797313Z","shell.execute_reply":"2022-05-26T19:23:11.07053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install mtcnn","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:14:13.892653Z","iopub.execute_input":"2022-05-27T12:14:13.892915Z","iopub.status.idle":"2022-05-27T12:14:27.097038Z","shell.execute_reply.started":"2022-05-27T12:14:13.892883Z","shell.execute_reply":"2022-05-27T12:14:27.096170Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# used to detect a face\nimport mtcnn\n# print version\nprint(mtcnn.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:14:39.045306Z","iopub.execute_input":"2022-05-27T12:14:39.045597Z","iopub.status.idle":"2022-05-27T12:14:39.498307Z","shell.execute_reply.started":"2022-05-27T12:14:39.045567Z","shell.execute_reply":"2022-05-27T12:14:39.497570Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# function for face detection with mtcnn\nfrom numpy import asarray\nfrom mtcnn.mtcnn import MTCNN\n\n# extract a single face from a given photograph\ndef extract_face(filename, required_size=(160, 160)):\n    # load image from file\n    image = Image.open(filename)\n    # convert to RGB, if needed\n    image = image.convert('RGB')\n    # convert to array\n    pixels = asarray(image)\n    # create the detector, using default weights\n    detector = MTCNN()\n    # detect faces in the image\n    results = detector.detect_faces(pixels)\n    # extract the bounding box from the first face\n    x1, y1, width, height = results[0]['box']\n    # bug fix\n    x1, y1 = abs(x1), abs(y1)\n    x2, y2 = x1 + width, y1 + height\n    # extract the face\n    face = pixels[y1:y2, x1:x2]\n    # resize pixels to the model size\n    image = Image.fromarray(face)\n    image = image.resize(required_size)\n    face_array = asarray(image)\n    return face_array","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:14:46.709812Z","iopub.execute_input":"2022-05-27T12:14:46.710542Z","iopub.status.idle":"2022-05-27T12:14:46.719623Z","shell.execute_reply.started":"2022-05-27T12:14:46.710502Z","shell.execute_reply":"2022-05-27T12:14:46.718746Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from os import listdir\n# load images and extract faces for a given subject\ndef load_faces(subject):\n    faces = list()\n    folder = '../input/yale-face-database/'\n    # enumerate files\n    for filename in listdir(folder):\n        if filename.split(\".\")[0] == 'subject' + subject:\n            # path\n            path = folder + filename\n            # get face\n            face = extract_face(path)\n            # store\n            faces.append(face)\n    return faces","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:14:51.165461Z","iopub.execute_input":"2022-05-27T12:14:51.166132Z","iopub.status.idle":"2022-05-27T12:14:51.172157Z","shell.execute_reply.started":"2022-05-27T12:14:51.166092Z","shell.execute_reply":"2022-05-27T12:14:51.171373Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# loads data and returns testing and training lists\ndef load_dataset():\n    subjects = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15']\n    X_train, y_train, X_test, y_test = list(), list(), list(), list()\n    count = 0\n    \n    for subject in subjects:\n        faces = load_faces(subject)\n        labels = [subject for _ in range(len(faces))]\n         # summarize progress\n        print('>loaded %d examples for class: %s' % (len(faces), subject))\n        # store\n\n        X_train.extend(faces[:7])\n        y_train.extend(labels[:7])\n        \n        X_test.extend(faces[7:])\n        y_test.extend(labels[7:])\n\n            \n            \n    return asarray(X_train), asarray(y_train), asarray(X_test), asarray(y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:14:54.299793Z","iopub.execute_input":"2022-05-27T12:14:54.300601Z","iopub.status.idle":"2022-05-27T12:14:54.308110Z","shell.execute_reply.started":"2022-05-27T12:14:54.300551Z","shell.execute_reply":"2022-05-27T12:14:54.307353Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from os.path import isdir\nfrom numpy import expand_dims\nfrom numpy import asarray\nfrom numpy import savez_compressed\n# load train and test data\ntrainX, trainy, testX, testy = load_dataset()\nprint(trainX.shape, trainy.shape)\n# save arrays to one file in compressed format\nsavez_compressed('./yale-face-database.npz', trainX, trainy, testX, testy)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:14:57.206864Z","iopub.execute_input":"2022-05-27T12:14:57.207416Z","iopub.status.idle":"2022-05-27T12:18:02.630652Z","shell.execute_reply.started":"2022-05-27T12:14:57.207377Z","shell.execute_reply":"2022-05-27T12:18:02.629887Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print(trainX.shape)\nprint(trainy.shape)\nprint(testX.shape)\nprint(testy.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:18:16.743199Z","iopub.execute_input":"2022-05-27T12:18:16.743678Z","iopub.status.idle":"2022-05-27T12:18:16.754587Z","shell.execute_reply.started":"2022-05-27T12:18:16.743629Z","shell.execute_reply":"2022-05-27T12:18:16.753108Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# pre computing face embeddings\nfrom numpy import load\nfrom numpy import expand_dims\nfrom numpy import asarray\nfrom numpy import savez_compressed\nfrom keras.models import load_model\n \n# get the face embedding for one face\ndef get_embedding(model, face_pixels):\n    # scale pixel values\n    face_pixels = face_pixels.astype('float32')\n    # standardize pixel values across channels (global)\n    mean, std = face_pixels.mean(), face_pixels.std()\n    face_pixels = (face_pixels - mean) / std\n    # transform face into one sample\n    samples = expand_dims(face_pixels, axis=0)\n    # make prediction to get embedding\n    yhat = model.predict(samples)\n    return yhat[0]","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:18:19.307883Z","iopub.execute_input":"2022-05-27T12:18:19.308636Z","iopub.status.idle":"2022-05-27T12:18:19.314878Z","shell.execute_reply.started":"2022-05-27T12:18:19.308599Z","shell.execute_reply":"2022-05-27T12:18:19.313872Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# load the face dataset\ndata = load('./yale-face-database.npz')\ntrainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\nprint('Loaded: ', trainX.shape, trainy.shape, testX.shape, testy.shape)\n# load the facenet model\nmodel = load_model('../input/facenet-keras/facenet_keras.h5')\nprint('Loaded Model')\n# convert each face in the train set to an embedding\nnewTrainX = list()\nfor face_pixels in trainX:\n    embedding = get_embedding(model, face_pixels)\n    newTrainX.append(embedding)\nnewTrainX = asarray(newTrainX)\nprint(newTrainX.shape)\n# convert each face in the test set to an embedding\nnewTestX = list()\nfor face_pixels in testX:\n    embedding = get_embedding(model, face_pixels)\n    newTestX.append(embedding)\nnewTestX = asarray(newTestX)\nprint(newTestX.shape)\n# save arrays to one file in compressed format\nsavez_compressed('./yale-faces-embeddings.npz', newTrainX, trainy, newTestX, testy)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:18:22.702362Z","iopub.execute_input":"2022-05-27T12:18:22.702629Z","iopub.status.idle":"2022-05-27T12:18:38.450526Z","shell.execute_reply.started":"2022-05-27T12:18:22.702598Z","shell.execute_reply":"2022-05-27T12:18:38.449697Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# perform classification\nfrom random import choice\nfrom numpy import load\nfrom numpy import expand_dims\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.svm import SVC\nfrom matplotlib import pyplot\n# load faces\n# data = load('./yale-face-database.npz')\n# testX_faces = data['arr_2']\n\n# load face embeddings\ndata = load('./yale-faces-embeddings.npz')\ntrainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n\n# normalize input vectors\nin_encoder = Normalizer(norm='l2')\ntrainX = in_encoder.transform(trainX)\ntestX = in_encoder.transform(testX)\n\n# label encode targets\nout_encoder = LabelEncoder()\nout_encoder.fit(trainy)\ntrainy = out_encoder.transform(trainy)\ntesty = out_encoder.transform(testy)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:18:49.485330Z","iopub.execute_input":"2022-05-27T12:18:49.485603Z","iopub.status.idle":"2022-05-27T12:18:49.887598Z","shell.execute_reply.started":"2022-05-27T12:18:49.485571Z","shell.execute_reply":"2022-05-27T12:18:49.886770Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.tree import DecisionTreeClassifier\n\nDTC = DecisionTreeClassifier(random_state=0)\nDTC.fit(trainX, trainy)\ny_pred = DTC.predict(testX)\nprint(\"accuracy score:{:.2f}\".format(accuracy_score(testy, y_pred)))\nprint(\"Classification Results:\\n{}\".format(classification_report(testy, y_pred)))","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:18:56.192732Z","iopub.execute_input":"2022-05-27T12:18:56.193317Z","iopub.status.idle":"2022-05-27T12:18:56.273954Z","shell.execute_reply.started":"2022-05-27T12:18:56.193258Z","shell.execute_reply":"2022-05-27T12:18:56.273166Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Eigen Faces - then KNN","metadata":{}},{"cell_type":"code","source":"from os import listdir\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# load images and extract faces for a given subject - updated version from the last model\ndef load_faces2(subject):\n    faces = list()\n    folder = '../input/yale-face-database/'\n    # enumerate files\n    for filename in listdir(folder):\n        if filename.split(\".\")[0] == 'subject' + subject:\n            # path\n            path = folder + filename\n            # get face\n            face = plt.imread(path)\n            # store\n            faces.append(face)\n    return faces","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:19:00.891768Z","iopub.execute_input":"2022-05-27T12:19:00.892310Z","iopub.status.idle":"2022-05-27T12:19:00.898235Z","shell.execute_reply.started":"2022-05-27T12:19:00.892268Z","shell.execute_reply":"2022-05-27T12:19:00.897235Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# updated version from the last model\ndef load_dataset2():\n    subjects = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15']\n    X_train, y_train, X_test, y_test = list(), list(), list(), list()\n    count = 0\n    \n    for subject in subjects:\n        faces = load_faces2(subject)\n        labels = [subject for _ in range(len(faces))]\n         # summarize progress\n        print('>loaded %d examples for class: %s' % (len(faces), subject))\n        # store\n\n        X_train.extend(faces[:8])\n        y_train.extend(labels[:8])\n        \n        X_test.extend(faces[8:])\n        y_test.extend(labels[8:])\n\n            \n            \n    return asarray(X_train), asarray(y_train), asarray(X_test), asarray(y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:19:04.727463Z","iopub.execute_input":"2022-05-27T12:19:04.727728Z","iopub.status.idle":"2022-05-27T12:19:04.735827Z","shell.execute_reply.started":"2022-05-27T12:19:04.727698Z","shell.execute_reply":"2022-05-27T12:19:04.734853Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"trainX1, trainy, testX1, testy = load_dataset2()\nprint(trainX.shape)\nprint(trainy.shape)\nprint(testX.shape)\nprint(testy.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:19:07.900848Z","iopub.execute_input":"2022-05-27T12:19:07.901172Z","iopub.status.idle":"2022-05-27T12:19:08.237163Z","shell.execute_reply.started":"2022-05-27T12:19:07.901139Z","shell.execute_reply":"2022-05-27T12:19:08.233921Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Reshaping images for machine learning model\ntrainX = trainX1.reshape((trainX1.shape[0],trainX1.shape[1]*trainX1.shape[2]))\nprint(\"New trainX shape:\",trainX.shape)\n\ntestX = testX1.reshape((testX1.shape[0],testX1.shape[1]*testX1.shape[2]))\nprint(\"New testX shape:\",testX.shape)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:19:12.224131Z","iopub.execute_input":"2022-05-27T12:19:12.224593Z","iopub.status.idle":"2022-05-27T12:19:12.231197Z","shell.execute_reply.started":"2022-05-27T12:19:12.224553Z","shell.execute_reply":"2022-05-27T12:19:12.230357Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# plotting a graph to see what the best value is for n_components\n\nfrom sklearn.decomposition import PCA\npca=PCA()\npca.fit(trainX)\n\nplt.figure(1, figsize=(12,8))\n\nplt.plot(pca.explained_variance_, linewidth=2)\n \nplt.xlabel('Components')\nplt.ylabel('Explained Variaces')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:19:16.859354Z","iopub.execute_input":"2022-05-27T12:19:16.859808Z","iopub.status.idle":"2022-05-27T12:19:20.108667Z","shell.execute_reply.started":"2022-05-27T12:19:16.859768Z","shell.execute_reply":"2022-05-27T12:19:20.107950Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# from the above diagram it can be seen that at around 40, the pca compenents represent the same data\nfrom sklearn.decomposition import PCA\npca=PCA(n_components=40, whiten=True)\npca.fit(trainX)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:19:25.095203Z","iopub.execute_input":"2022-05-27T12:19:25.096132Z","iopub.status.idle":"2022-05-27T12:19:26.750326Z","shell.execute_reply.started":"2022-05-27T12:19:25.096080Z","shell.execute_reply":"2022-05-27T12:19:26.749267Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# the average face of the dataset\n\nfig,ax=plt.subplots(1,1,figsize=(8,8))\nax.imshow(pca.mean_.reshape((243,320)), cmap=\"gray\")\nax.set_xticks([])\nax.set_yticks([])\nax.set_title('Average Face')","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:19:30.236749Z","iopub.execute_input":"2022-05-27T12:19:30.237242Z","iopub.status.idle":"2022-05-27T12:19:30.410616Z","shell.execute_reply.started":"2022-05-27T12:19:30.237199Z","shell.execute_reply":"2022-05-27T12:19:30.409901Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# pictures of some of the faces after encoding\n\nnumber_of_eigenfaces=len(pca.components_)\neigen_faces=pca.components_.reshape((number_of_eigenfaces, trainX1.shape[1], trainX1.shape[2]))\n\ncols=10\nrows=int(number_of_eigenfaces/cols)\nfig, axarr=plt.subplots(nrows=rows, ncols=cols, figsize=(15,15))\naxarr=axarr.flatten()\nfor i in range(number_of_eigenfaces):\n    axarr[i].imshow(eigen_faces[i],cmap=\"gray\")\n    axarr[i].set_xticks([])\n    axarr[i].set_yticks([])\n    axarr[i].set_title(\"eigen id:{}\".format(i))\nplt.suptitle(\"All Eigen Faces\".format(10*\"=\", 10*\"=\"))","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:19:34.128796Z","iopub.execute_input":"2022-05-27T12:19:34.129535Z","iopub.status.idle":"2022-05-27T12:19:36.203197Z","shell.execute_reply.started":"2022-05-27T12:19:34.129495Z","shell.execute_reply":"2022-05-27T12:19:36.199290Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"X_train_pca=pca.transform(trainX)\nX_test_pca=pca.transform(testX)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:19:42.776164Z","iopub.execute_input":"2022-05-27T12:19:42.776793Z","iopub.status.idle":"2022-05-27T12:19:42.907485Z","shell.execute_reply.started":"2022-05-27T12:19:42.776754Z","shell.execute_reply":"2022-05-27T12:19:42.906487Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nKNN = KNeighborsClassifier(n_neighbors=1)\nKNN.fit(X_train_pca, trainy)\ny_pred = KNN.predict(X_test_pca)\nprint(\"accuracy score:{:.2f}\".format(accuracy_score(testy, y_pred)))\nprint(\"Classification Results:\\n{}\".format(classification_report(testy, y_pred)))","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:19:45.752700Z","iopub.execute_input":"2022-05-27T12:19:45.752976Z","iopub.status.idle":"2022-05-27T12:19:45.772099Z","shell.execute_reply.started":"2022-05-27T12:19:45.752926Z","shell.execute_reply":"2022-05-27T12:19:45.771286Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-05-26T11:07:03.475979Z","iopub.status.idle":"2022-05-26T11:07:03.476537Z","shell.execute_reply.started":"2022-05-26T11:07:03.476304Z","shell.execute_reply":"2022-05-26T11:07:03.476331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Neural Network Model","metadata":{}},{"cell_type":"code","source":"# used for face detection\n!pip install mtcnn","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:19:53.125438Z","iopub.execute_input":"2022-05-27T12:19:53.126044Z","iopub.status.idle":"2022-05-27T12:20:02.357658Z","shell.execute_reply.started":"2022-05-27T12:19:53.126008Z","shell.execute_reply":"2022-05-27T12:20:02.356774Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"import os","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:20:09.125404Z","iopub.execute_input":"2022-05-27T12:20:09.125699Z","iopub.status.idle":"2022-05-27T12:20:09.129768Z","shell.execute_reply.started":"2022-05-27T12:20:09.125669Z","shell.execute_reply":"2022-05-27T12:20:09.128987Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Imports\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom PIL import Image\n\n# Confirm mtcnn was installed correctly\nimport mtcnn\nfrom mtcnn.mtcnn import MTCNN\nfrom matplotlib.patches import Rectangle\n\nfrom os import listdir\nfrom tqdm import tqdm\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:20:13.240257Z","iopub.execute_input":"2022-05-27T12:20:13.240652Z","iopub.status.idle":"2022-05-27T12:20:13.397543Z","shell.execute_reply.started":"2022-05-27T12:20:13.240617Z","shell.execute_reply":"2022-05-27T12:20:13.396768Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"DIRECTORY = \"../input/yale-face-database/\"","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:20:25.521255Z","iopub.execute_input":"2022-05-27T12:20:25.521738Z","iopub.status.idle":"2022-05-27T12:20:25.525484Z","shell.execute_reply.started":"2022-05-27T12:20:25.521708Z","shell.execute_reply":"2022-05-27T12:20:25.524740Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"filename = \"../input/yale-face-database/subject01.centerlight\"\npixels = plt.imread(filename)\n\nrgb_pixels = np.stack((pixels, pixels, pixels), axis=2)\nprint(rgb_pixels.shape)\nplt.imshow(pixels)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:20:27.657864Z","iopub.execute_input":"2022-05-27T12:20:27.658626Z","iopub.status.idle":"2022-05-27T12:20:27.867585Z","shell.execute_reply.started":"2022-05-27T12:20:27.658578Z","shell.execute_reply":"2022-05-27T12:20:27.866878Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# detecting facial features in the pictures\ndetector = MTCNN()\nresults = detector.detect_faces(rgb_pixels)\nresults","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:21:07.863585Z","iopub.execute_input":"2022-05-27T12:21:07.864044Z","iopub.status.idle":"2022-05-27T12:21:08.901563Z","shell.execute_reply.started":"2022-05-27T12:21:07.864000Z","shell.execute_reply":"2022-05-27T12:21:08.900727Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# function to find a face and draw box round it\ndef draw_image_with_boxes(data, result_list):\n    # plot the image\n    plt.imshow(data)\n    # get the context for drawing boxes\n    ax = plt.gca()\n    # plot each box\n    for result in result_list:\n        # get coordinates\n        x, y, width, height = result['box']\n        # create the shape\n        rect = Rectangle((x, y), width, height, fill=False, color='red')\n        # draw the box\n        ax.add_patch(rect)\n    # show the plot\n    plt.show()\n\n# display faces on the original image\ndraw_image_with_boxes(rgb_pixels, results)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:21:12.145319Z","iopub.execute_input":"2022-05-27T12:21:12.145792Z","iopub.status.idle":"2022-05-27T12:21:12.347394Z","shell.execute_reply.started":"2022-05-27T12:21:12.145753Z","shell.execute_reply":"2022-05-27T12:21:12.346714Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# extract a single face from a given photograph\ndef extract_face_from_file(filename, required_size=(160, 160)):\n    # load image from file\n    image = Image.open(filename)\n    \n    return extract_face(image, required_size)\n\ndef extract_face(image, required_size=(160, 160)):\n    # convert to RGB, if needed\n    image = image.convert('RGB')\n    # convert to array\n    pixels = np.asarray(image)\n    # detect faces in the image\n    results = detector.detect_faces(pixels)\n    # extract the bounding box from the first face\n    x1, y1, width, height = results[0]['box']\n    # bug fix\n    x1, y1 = abs(x1), abs(y1)\n    x2, y2 = x1 + width, y1 + height\n    # extract the face\n    face = pixels[y1:y2, x1:x2]\n    # resize pixels to the model size\n    image = Image.fromarray(face)\n    image = image.resize(required_size)\n    face_array = np.asarray(image)\n    gray_face = cv2.cvtColor(face_array, cv2.COLOR_BGR2GRAY)\n    \n    return gray_face\n\n\n# Create the detector, using default weights\ndetector = MTCNN()\n\n# load the photo and extract the face\nface_pixels = extract_face_from_file(\"../input/yale-face-database/subject01.centerlight\")\n\nplt.imshow(face_pixels)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:21:16.568705Z","iopub.execute_input":"2022-05-27T12:21:16.569602Z","iopub.status.idle":"2022-05-27T12:21:18.160516Z","shell.execute_reply.started":"2022-05-27T12:21:16.569553Z","shell.execute_reply":"2022-05-27T12:21:18.159701Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"def list_files(directory, contains):\n    return list(f for f in listdir(directory) if contains in f)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:21:23.769186Z","iopub.execute_input":"2022-05-27T12:21:23.769653Z","iopub.status.idle":"2022-05-27T12:21:23.773658Z","shell.execute_reply.started":"2022-05-27T12:21:23.769614Z","shell.execute_reply":"2022-05-27T12:21:23.772995Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"i = 1\nfaces = list()\nfor filename in tqdm(list_files(DIRECTORY, \"subject\")[0:16]):\n    # path\n    path = DIRECTORY + filename\n    # get face\n    face = extract_face_from_file(path)\n    # plot\n    plt.subplot(4, 4, i)\n    plt.axis('off')\n    plt.imshow(face)\n    faces.append(face)\n    i += 1\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:21:26.866674Z","iopub.execute_input":"2022-05-27T12:21:26.866941Z","iopub.status.idle":"2022-05-27T12:21:33.743020Z","shell.execute_reply.started":"2022-05-27T12:21:26.866900Z","shell.execute_reply":"2022-05-27T12:21:33.742302Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# list filenames\nfilenames = pd.DataFrame(list_files(DIRECTORY, \"subject\"))\n\n# generate split \ndf = filenames[0].str.split(\".\", expand=True)\ndf[\"filename\"] = filenames\n\n# # tidy columns\ndf = df.rename(columns = {0:\"subject\", 1:\"category\"})\ndf['subject'] = df.subject.str.replace('subject' , '')\ndf.apply(pd.to_numeric, errors='coerce').dropna()\ndf['subject'] = pd.to_numeric(df[\"subject\"])\ndf","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:21:38.134605Z","iopub.execute_input":"2022-05-27T12:21:38.134888Z","iopub.status.idle":"2022-05-27T12:21:38.172780Z","shell.execute_reply.started":"2022-05-27T12:21:38.134857Z","shell.execute_reply":"2022-05-27T12:21:38.171880Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"PER_CLASS = 8 # 11 images (3 test & 8 train)\nNO_CLASSES = 15\nDS_SIZE = df[\"subject\"].count()\nTEST_SIZE = 1 - (PER_CLASS * NO_CLASSES / DS_SIZE)\n\n# # list files for each group\n# # df.groupby(['subject'])['filename'].apply(list)\ny = df['subject']\nX = df.drop('subject',axis=1)\n\n# # subject\nX_train_info, X_test_info, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=45, stratify=y)\n\ny_train = y_train.tolist()\ny_test = y_test.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:21:42.063811Z","iopub.execute_input":"2022-05-27T12:21:42.064505Z","iopub.status.idle":"2022-05-27T12:21:42.075718Z","shell.execute_reply.started":"2022-05-27T12:21:42.064465Z","shell.execute_reply":"2022-05-27T12:21:42.074626Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# detector = MTCNN()\n\ndef load_dataset(dataset):\n    faces = list()\n    for filename in tqdm(dataset[\"filename\"]):\n        path = DIRECTORY + filename\n        # get face\n        face = extract_face_from_file(path)\n        faces.append(face)\n    return np.asarray(faces)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:21:45.928661Z","iopub.execute_input":"2022-05-27T12:21:45.928918Z","iopub.status.idle":"2022-05-27T12:21:45.933927Z","shell.execute_reply.started":"2022-05-27T12:21:45.928888Z","shell.execute_reply":"2022-05-27T12:21:45.933033Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"X_test = load_dataset(X_test_info)\nX_train = load_dataset(X_train_info)\n\nprint(X_test.shape)\nprint(X_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:21:48.714420Z","iopub.execute_input":"2022-05-27T12:21:48.714910Z","iopub.status.idle":"2022-05-27T12:22:52.045276Z","shell.execute_reply.started":"2022-05-27T12:21:48.714874Z","shell.execute_reply":"2022-05-27T12:22:52.044515Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# Options \n\nTRAINING_DATA_DIRECTORY = \"data/train\"\nTESTING_DATA_DIRECTORY = \"data/test\"\nNUM_CLASSES = 15\nEPOCHS = 25\nBATCH_SIZE = 5\nNUMBER_OF_TRAINING_IMAGES = 120\nNUMBER_OF_TESTING_IMAGES = 45\nIMAGE_HEIGHT = 160\nIMAGE_WIDTH = 160","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:23:33.669624Z","iopub.execute_input":"2022-05-27T12:23:33.669889Z","iopub.status.idle":"2022-05-27T12:23:33.674579Z","shell.execute_reply.started":"2022-05-27T12:23:33.669860Z","shell.execute_reply":"2022-05-27T12:23:33.673888Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"import os \n\ndef save_keras_dataset(setname, dataset, labels, per_class):\n    # combine labels and images to generate files\n    data = sorted(list(zip(labels, dataset)), key=lambda x: x[0])\n\n    # Save images\n    j = 0\n    for label, gray_img in tqdm(data):\n        j = (j% per_class) + 1\n        # Create directory\n        directory = f\"data/{setname}/class_{label}/\"\n        if not os.path.exists(directory):\n                os.makedirs(directory)\n        cv2.imwrite(f\"{directory}class_{label}_{j}.png\",gray_img)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:23:36.333082Z","iopub.execute_input":"2022-05-27T12:23:36.333623Z","iopub.status.idle":"2022-05-27T12:23:36.340539Z","shell.execute_reply.started":"2022-05-27T12:23:36.333584Z","shell.execute_reply":"2022-05-27T12:23:36.339853Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# clear directory if it already exists\nimport shutil\nshutil.rmtree(r'data', ignore_errors=True)\n\n# Save datasets\nsave_keras_dataset(\"test\", X_test, y_test, 3)\nsave_keras_dataset(\"train\", X_train, y_train, 8)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:23:39.602265Z","iopub.execute_input":"2022-05-27T12:23:39.602550Z","iopub.status.idle":"2022-05-27T12:23:39.695842Z","shell.execute_reply.started":"2022-05-27T12:23:39.602522Z","shell.execute_reply":"2022-05-27T12:23:39.694994Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\ndef data_generator():\n    return ImageDataGenerator(\n          rescale=1./255,\n#           horizontal_flip=True,\n#         fill_mode=\"nearest\",\n#         zoom_range=0.1,\n#         width_shift_range=0.1,\n#         height_shift_range=0.1,\n#         rotation_range=10,\n#         preprocessing_function=add_noise\n    )\n\ndef add_noise(img):\n    \"\"\"Add random noise to an image\"\"\"\n    VARIABILITY = 35\n    deviation = VARIABILITY*random.random()\n    noise = np.random.normal(0, deviation, img.shape)\n    img += noise\n    np.clip(img, 0., 255.)\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:23:43.582557Z","iopub.execute_input":"2022-05-27T12:23:43.582812Z","iopub.status.idle":"2022-05-27T12:23:43.591731Z","shell.execute_reply.started":"2022-05-27T12:23:43.582784Z","shell.execute_reply":"2022-05-27T12:23:43.590763Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# Setup Data Generators\ntraining_generator = data_generator().flow_from_directory(\n    TRAINING_DATA_DIRECTORY,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    color_mode='grayscale'\n)\n\ntesting_generator = data_generator().flow_from_directory(\n    TESTING_DATA_DIRECTORY,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    class_mode='categorical',\n    color_mode='grayscale'\n)\n\nvalidation_generator = data_generator().flow_from_directory(\n    TESTING_DATA_DIRECTORY,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    class_mode='categorical',\n    color_mode='grayscale',\n    shuffle=False # IMPORTANT: to ensure classes line up with batches\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:23:47.209534Z","iopub.execute_input":"2022-05-27T12:23:47.209808Z","iopub.status.idle":"2022-05-27T12:23:47.521381Z","shell.execute_reply.started":"2022-05-27T12:23:47.209781Z","shell.execute_reply":"2022-05-27T12:23:47.520685Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"import random\nsample_images = testing_generator.next()[0]\n\nf, xyarr = plt.subplots(3,3)\nxyarr[0,0].imshow(sample_images[0])\nxyarr[0,1].imshow(sample_images[1])\nxyarr[0,2].imshow(sample_images[2])\nxyarr[1,0].imshow(sample_images[3])\nxyarr[1,1].imshow(sample_images[4])\nxyarr[1,2].imshow(sample_images[5])\nxyarr[2,0].imshow(sample_images[6])\nxyarr[2,1].imshow(sample_images[7])\nxyarr[2,2].imshow(sample_images[8])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:23:51.153945Z","iopub.execute_input":"2022-05-27T12:23:51.154493Z","iopub.status.idle":"2022-05-27T12:23:51.873620Z","shell.execute_reply.started":"2022-05-27T12:23:51.154455Z","shell.execute_reply":"2022-05-27T12:23:51.872878Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import keras\nclass MCDropout(keras.layers.Dropout):\n    def call(self, inputs):\n        return super().call(inputs, training=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:23:58.855465Z","iopub.execute_input":"2022-05-27T12:23:58.855734Z","iopub.status.idle":"2022-05-27T12:23:58.860263Z","shell.execute_reply.started":"2022-05-27T12:23:58.855702Z","shell.execute_reply":"2022-05-27T12:23:58.859468Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import models\nfrom tensorflow.keras.layers import Activation, ZeroPadding2D, MaxPooling2D, Conv2D, Flatten, Dense, Dropout\nfrom tensorflow.keras import regularizers, constraints\n\n# Model\nmodel = models.Sequential()\n\n# Convolution layers\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 1), padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(64, (2, 2), activation='relu', padding='same'))\nmodel.add(MaxPooling2D(2))\n\nmodel.add(Conv2D(64, (2, 2), activation='relu', padding='same'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Flatten the Convolution\nmodel.add(Flatten())\n\n# dense layer\nmodel.add(Dense(units = 64, activation='relu'))\n\n# drop layer to reduce overfitting\nmodel.add(Dropout(rate=0.2))\n\n# Final output layer\nmodel.add(Dense(NUM_CLASSES, activation='softmax'))\n          \nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:24:16.310100Z","iopub.execute_input":"2022-05-27T12:24:16.310612Z","iopub.status.idle":"2022-05-27T12:24:16.380615Z","shell.execute_reply.started":"2022-05-27T12:24:16.310572Z","shell.execute_reply":"2022-05-27T12:24:16.379952Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import optimizers, losses\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping()\n\nmodel.compile(\n    loss=losses.CategoricalCrossentropy(),\n    optimizer=optimizers.Adam(learning_rate=0.0003),\n    metrics=[\"accuracy\"]\n)\n\n# from_logits=True\n\nhistory = model.fit(\n    training_generator,\n    steps_per_epoch=(NUMBER_OF_TRAINING_IMAGES//BATCH_SIZE ),\n    epochs=EPOCHS,\n    validation_data=testing_generator,\n    shuffle=True,\n    validation_steps=(NUMBER_OF_TESTING_IMAGES//BATCH_SIZE),\n#     callbacks=[early_stopping]\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:24:20.150097Z","iopub.execute_input":"2022-05-27T12:24:20.150697Z","iopub.status.idle":"2022-05-27T12:24:26.568604Z","shell.execute_reply.started":"2022-05-27T12:24:20.150661Z","shell.execute_reply":"2022-05-27T12:24:26.567836Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\nfrom sklearn.metrics import accuracy_score\n\nY_pred = model.predict(validation_generator)\ny_pred = np.argmax(Y_pred, axis=1)\nprint(\"classification report\")\nprint(classification_report(validation_generator.classes, y_pred))\nprint(\"average:\")\nprint(accuracy_score(validation_generator.classes, y_pred))\n\n# i = 0\n# average = 0\n# while i < 10:\n#     Y_pred = model.predict(validation_generator)\n#     y_pred = np.argmax(Y_pred, axis=1)\n    \n#     average = average + accuracy_score(validation_generator.classes, y_pred)\n#     i = i + 1\n    \n# average = average / 10\n# print(\"average:\")\n# print(average)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-27T12:24:30.053150Z","iopub.execute_input":"2022-05-27T12:24:30.053675Z","iopub.status.idle":"2022-05-27T12:24:30.223967Z","shell.execute_reply.started":"2022-05-27T12:24:30.053623Z","shell.execute_reply":"2022-05-27T12:24:30.223107Z"},"trusted":true},"execution_count":54,"outputs":[]}]}